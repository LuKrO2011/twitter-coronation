{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets analysis\n",
    "\n",
    "This notebook is used to analyse tweets.\n",
    "\n",
    "Note that this it requires already downloaded (and pre-processed) tweets. This can be done by using twitterDownloader.ipynb (and twitterFilterer.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import TwitterProcessing\n",
    "\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import ssl\n",
    "\n",
    "import pyLDAvis\n",
    "from pyLDAvis import lda_model\n",
    "\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some warnings with LDA which we want to ignore\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(text, tokenizer=nltk.tokenize.TweetTokenizer(), stemmer=nltk.stem.PorterStemmer(), stopwords=[]):\n",
    "    \"\"\"\n",
    "    Perform tokenization, normalisation (lower case and stemming) and stopword and twitter keyword removal.\n",
    "\n",
    "    @param text: tweet text\n",
    "    @param tokenizer: tokenizer used.\n",
    "    @param stemmer: stemmer used.\n",
    "    @param stopwords: list of stopwords used\n",
    "\n",
    "    @returns: a list of processed tokens\n",
    "    \"\"\"\n",
    "    # covert all to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # strip whitespaces before and after\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    # stem (we use set to remove duplicates)\n",
    "    stemmed_tokens = set([stemmer.stem(tok) for tok in tokens])\n",
    "\n",
    "    # remove stopwords, digits\n",
    "    return [tok for tok in stemmed_tokens if tok not in stopwords and not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words(filename):\n",
    "    \"\"\"\n",
    "    Loads the worlds of the file with the given name to a set.\n",
    "    :param filename: The name of the file to load the words from.\n",
    "    :return: A set of words loaded from the file.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    with codecs.open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            words.append(line.strip())\n",
    "\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(filename):\n",
    "    \"\"\"\n",
    "    Loads the tweets from the file with the given name into an array of tweets.\n",
    "\n",
    "    @param filename: The filename of the file to load the tweets from.\n",
    "\n",
    "    @returns: An array of tweets.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for sLine in f:\n",
    "            tweet = json.loads(sLine)\n",
    "            tweets.append(tweet)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    \"\"\"\n",
    "    Extracts the associated hashtags of tweet.\n",
    "\n",
    "    @param tweet: The tweet, which is in the tweepy json format, and which we wish to extract its associated hashtags.\n",
    "\n",
    "    @returns: list of hashtags (in lower case)\n",
    "    \"\"\"\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('hashtags', [])\n",
    "\n",
    "    return [tag['tag'].lower() for tag in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the json file with the tweets to process\n",
    "tweets_filename = '../data/coronation_2023_05_05_filtered.json'\n",
    "\n",
    "# number of hashtags to display\n",
    "hashtag_trash = 50\n",
    "\n",
    "# number of most used words to display\n",
    "words_trash = 50\n",
    "\n",
    "# Load the tweets\n",
    "tweets = load_tweets(tweets_filename)\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of tweets.\n",
    "print(\"Total number of tweets: \", len(tweets))\n",
    "\n",
    "first_created_at = tweets[0].get('created_at', '')\n",
    "for tweet in tweets:\n",
    "    created_at = tweet.get('created_at', '')\n",
    "    if created_at is not None or first_created_at < created_at:\n",
    "        last_created_at = created_at\n",
    "print(\"First tweets date: \", first_created_at)\n",
    "\n",
    "last_created_at = tweets[0].get('created_at', '')\n",
    "for tweet in tweets:\n",
    "    created_at = tweet.get('created_at', '')\n",
    "    if created_at is not None and last_created_at > created_at:\n",
    "        last_created_at = created_at\n",
    "print(\"Last tweets date: \", last_created_at)\n",
    "\n",
    "first_datetime = datetime.fromtimestamp(mktime(strptime(first_created_at, \"%Y-%m-%dT%H:%M:%S.000Z\")))\n",
    "last_datetime = datetime.fromtimestamp(mktime(strptime(last_created_at, \"%Y-%m-%dT%H:%M:%S.000Z\")))\n",
    "\n",
    "timediff =  first_datetime - last_datetime\n",
    "print(\"Time difference: \" + timediff.__str__())\n",
    "\n",
    "tweets_per_second = len(tweets)/timediff.total_seconds()\n",
    "print(\"Tweets per second: \" + tweets_per_second.__str__())\n",
    "\n",
    "seconds_per_week = 604800\n",
    "estimated_max_timeframe_tweets = int(tweets_per_second * seconds_per_week)\n",
    "print(\"Tweet estimation for one week: \" + estimated_max_timeframe_tweets.__str__())\n",
    "\n",
    "seconds_per_day = 86400\n",
    "estimated_tweets_per_day = int(tweets_per_second * seconds_per_day)\n",
    "print(\"Tweet estimation for one day \" + estimated_tweets_per_day.__str__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Keyword and hashtag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a counter for counting hashtags\n",
    "hashtag_counter = Counter()\n",
    "\n",
    "# Add hashtags to counter\n",
    "for tweet in tweets:\n",
    "    hashtagsInTweet = get_hashtags(tweet)\n",
    "    hashtag_counter.update(hashtagsInTweet)\n",
    "\n",
    "# Print most used hashtags\n",
    "for tag, count in hashtag_counter.most_common(hashtag_trash):\n",
    "    print(tag + \": \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet tokenizer to use\n",
    "tweet_tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# Use the punctuation symbols defined in string.punctuation\n",
    "puncts = list(string.punctuation)\n",
    "\n",
    "# Use stopwords from nltk and a few other twitter specific terms\n",
    "twitter_stopwords = ['...', '‚Ä¶', '\"', \"'\", '`', '‚Äò', '‚Äú', '‚Äù',' ','re']\n",
    "web_stopwords = ['uk','üá¨','üáß','https','co']\n",
    "all_stopwords = nltk.corpus.stopwords.words('english')\\\n",
    "                + twitter_stopwords\\\n",
    "                + web_stopwords \\\n",
    "                + puncts\n",
    "stopwords = list(dict.fromkeys(all_stopwords))\n",
    "\n",
    "# Use the popular Porter stemmer\n",
    "tweet_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# The term frequency counter\n",
    "word_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_processed = []\n",
    "for tweet in tweets:\n",
    "    tweet_text = tweet.get('text', '')\n",
    "\n",
    "    # Tokenize, filter stopwords and get convert to lower case\n",
    "    tokens = process_tweet(text=tweet_text, tokenizer=tweet_tokenizer, stemmer=tweet_stemmer, stopwords=stopwords)\n",
    "    tweet_text_processed.append(' '.join(tokens))\n",
    "\n",
    "    # Update counter\n",
    "    word_counter.update(tokens)\n",
    "\n",
    "# Print out most common terms\n",
    "for term, count in word_counter.most_common(words_trash):\n",
    "    print(term + ': ' + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment_analysis(tweet_filename, tweet_processor, preprocess_words=True):\n",
    "    \"\"\"\n",
    "    Use Vader lexicons instead of a raw positive and negative word count.\n",
    "\n",
    "    @param tweet_filename: name of input file containing a json formatted tweet dump\n",
    "    @param tweet_processor: TweetProcessing object, used to pre-process each tweet.\n",
    "    @param preprocess_words: Whether the words should be preprocessed before analysis or not.\n",
    "\n",
    "    @returns: list of tweets, in the format of [date, sentiment]\n",
    "    \"\"\"\n",
    "    # this is the vader sentiment analyser, part of nltk\n",
    "    sent_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    sentiments = []\n",
    "    # open file and process tweets, one by one\n",
    "    with open(tweet_filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # each line is loaded according to json format, into tweet, which is actually a dictionary\n",
    "            tweet = json.loads(line)\n",
    "\n",
    "            try:\n",
    "                tweet_text = tweet.get('text', '')\n",
    "                tweet_date = tweet.get('created_at')\n",
    "\n",
    "                # pre-process the tweet text\n",
    "                if preprocess_words:\n",
    "                    tokens = tweet_processor.process(tweet_text)\n",
    "                else:\n",
    "                    tokens = tweet_text\n",
    "\n",
    "                # this computes the sentiment scores (called polarity score in nltk, but mean same thing essentially)\n",
    "                # see lab sheet for what dSentimentScores holds\n",
    "                if preprocess_words:\n",
    "                    sentiment_scores = sent_analyser.polarity_scores(\" \".join(tokens))\n",
    "                else:\n",
    "                    sentiment_scores = sent_analyser.polarity_scores(tokens)\n",
    "\n",
    "                # save the date and sentiment of each tweet (used for time series)\n",
    "                sentiments.append([pd.to_datetime(tweet_date), sentiment_scores['compound']])\n",
    "\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the TwitterProcessing python script\n",
    "tweet_processor = TwitterProcessing.TwitterProcessing(tweet_tokenizer, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the sentiment\n",
    "sentiments = vader_sentiment_analysis(tweets_filename, tweet_processor, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gathered data\n",
    "series = pd.DataFrame(sentiments, columns=['date', 'sentiment'])\n",
    "series.set_index('date', inplace=True)\n",
    "series[['sentiment']] = series[['sentiment']].apply(pd.to_numeric)\n",
    "\n",
    "series = series.resample('1h').sum() # mean\n",
    "series.plot()\n",
    "plt.title(\"Sentiment over time [per hour]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gathered data\n",
    "# pd.date_range(\"2022-10-06\", \"2022-10-07\")\n",
    "\n",
    "series = pd.DataFrame(sentiments, columns=['date', 'sentiment'])\n",
    "series[['sentiment']] = series[['sentiment']].apply(pd.to_numeric)\n",
    "\n",
    "series.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "series = series.resample('15min').sum()\n",
    "series.plot()\n",
    "plt.title(\"Sentiment over time [per 15min]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, words_to_print_per_topic):\n",
    "    \"\"\"\n",
    "    Prints out the most associated words for each feature.\n",
    "\n",
    "    @param model: lda model.\n",
    "    @param feature_names: list of strings, representing the list of features/words.\n",
    "    @param words_to_print_per_topic: number of words to print per topic.\n",
    "    \"\"\"\n",
    "\n",
    "    # print out the topic distributions\n",
    "    for topic_id, topic_distribution in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_id+1))\n",
    "        print(\" \".join([feature_names[i] for i in topic_distribution.argsort()[:-words_to_print_per_topic - 1:-1]]))\n",
    "\n",
    "def display_word_crowd(model, feature_names):\n",
    "    \"\"\"\n",
    "    Displays the word cloud of the topic distributions of the model.\n",
    "\n",
    "    @param model: The LDA model.\n",
    "    @param feature_names: list of strings, representing the list of features/words.\n",
    "    \"\"\"\n",
    "\n",
    "    # normalize each row/topic to sum to one\n",
    "    normalised_components = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Display a wordcrowd for each topic\n",
    "    for topic_id, topic_distribution in enumerate(normalised_components):\n",
    "        word_probabilities = {feature_names[i] : wordProb for i, wordProb in enumerate(topic_distribution)}\n",
    "        wordcloud = WordCloud(background_color='black')\n",
    "        wordcloud.fit_words(frequencies=word_probabilities)\n",
    "        plt.title('Topic %d:' % (topic_id+1))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics to discover (default = 10)\n",
    "number_of_topics = 5\n",
    "\n",
    "# maximum number of words to display per topic (default = 10)\n",
    "words_to_display_per_topic = 10\n",
    "\n",
    "# number of features/words to describe our documents\n",
    "number_of_features = 1500\n",
    "\n",
    "# extract a document-term matrix and the feature names using a CountVectorizer to do counting\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=number_of_features, stop_words=stopwords)\n",
    "document_term_matrix = vectorizer.fit_transform(tweet_text_processed)\n",
    "\n",
    "# extract the names of the features (= words)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model with the data\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, max_iter=10, learning_method='online').fit(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the most common words per topic.\n",
    "display_topics(model, feature_names, words_to_display_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using pyLDAvis\n",
    "panel = pyLDAvis.lda_model.prepare(model, document_term_matrix ,vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the wordclouds\n",
    "display_word_crowd(model, feature_names)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
